# -*- coding: utf-8 -*-
"""Customer Churn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1B1a0R-2CfXYAwNu9dV1ie4YSE1aq4x8c
"""

# ============================================================
# üöÄ CUSTOMER CHURN MODEL COMPARISON (Auto-Tuned + Export)
# ============================================================

!pip install catboost lightgbm xgboost scikit-learn pandas numpy seaborn matplotlib

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier
from xgboost import XGBClassifier

# ============================================================
# 1Ô∏è‚É£ Load & Prepare Data
# ============================================================
df = pd.read_csv("/content/Churn_Modelling.csv")

# Keep CustomerId separately for later mapping
customer_ids = df["CustomerId"]

# Drop irrelevant columns
df.drop(["RowNumber", "CustomerId", "Surname"], axis=1, inplace=True)

# Encode categorical features
df["Geography"] = LabelEncoder().fit_transform(df["Geography"])
df["Gender"] = LabelEncoder().fit_transform(df["Gender"])

# ============================================================
# 2Ô∏è‚É£ Feature Engineering
# ============================================================
df["BalanceSalaryRatio"] = df["Balance"] / (df["EstimatedSalary"] + 1)
df["TenureByAge"] = df["Tenure"] / (df["Age"] + 1)
df["CreditScoreAgeRatio"] = df["CreditScore"] / (df["Age"] + 1)

# ============================================================
# 3Ô∏è‚É£ Split Data
# ============================================================
X = df.drop("Exited", axis=1)
y = df["Exited"]

X_train, X_test, y_train, y_test, id_train, id_test = train_test_split(
    X, y, customer_ids, test_size=0.2, random_state=42, stratify=y
)

# ============================================================
# 4Ô∏è‚É£ Correlation Heatmap
# ============================================================
plt.figure(figsize=(12,7))
sns.heatmap(df.corr(), annot=False, cmap="coolwarm")
plt.title("Feature Correlation Heatmap")
plt.show()

# ============================================================
# 5Ô∏è‚É£ CatBoost Auto-Tuning
# ============================================================
best_model = None
best_acc = 0
best_params = {}

for depth in [6, 8, 10]:
    for lr in [0.01, 0.05, 0.1]:
        for l2 in [1, 3, 5]:
            model = CatBoostClassifier(
                iterations=1500,
                learning_rate=lr,
                depth=depth,
                l2_leaf_reg=l2,
                cat_features=[0,1],
                verbose=False,
                eval_metric='AUC',
                random_state=42
            )
            model.fit(X_train, y_train, eval_set=(X_test, y_test),
                      early_stopping_rounds=50)
            preds = model.predict(X_test)
            acc = accuracy_score(y_test, preds)
            if acc > best_acc:
                best_acc = acc
                best_model = model
                best_params = {"depth": depth, "lr": lr, "l2": l2}

print(f"\n‚úÖ Best CatBoost Accuracy: {best_acc:.4f} with params {best_params}")

# ============================================================
# 6Ô∏è‚É£ Compare with LightGBM & XGBoost
# ============================================================
models = {
    "CatBoost": best_model,
    "LightGBM": LGBMClassifier(
        n_estimators=1200, learning_rate=0.05, max_depth=8,
        subsample=0.8, colsample_bytree=0.8, random_state=42
    ),
    "XGBoost": XGBClassifier(
        n_estimators=1200, learning_rate=0.05, max_depth=8,
        subsample=0.8, colsample_bytree=0.8, eval_metric="logloss",
        random_state=42, use_label_encoder=False
    )
}

results = []

for name, model in models.items():
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    probs = model.predict_proba(X_test)[:,1]

    acc = accuracy_score(y_test, preds)
    prec = precision_score(y_test, preds)
    rec = recall_score(y_test, preds)
    f1 = f1_score(y_test, preds)
    auc = roc_auc_score(y_test, probs)

    results.append([name, acc, prec, rec, f1, auc])

# ============================================================
# 7Ô∏è‚É£ Evaluation Table
# ============================================================
results_df = pd.DataFrame(
    results,
    columns=["Model", "Accuracy", "Precision", "Recall", "F1", "AUC"]
).sort_values(by="Accuracy", ascending=False)

print("\nüîπ Model Comparison Results:")
print(results_df)

# ============================================================
# 8Ô∏è‚É£ Feature Importance (for Best Model)
# ============================================================
best_name = results_df.iloc[0]["Model"]
best_model = models[best_name]

if best_name == "CatBoost":
    feat_imp = pd.DataFrame({
        "Feature": X.columns,
        "Importance": best_model.get_feature_importance()
    }).sort_values(by="Importance", ascending=False)
else:
    feat_imp = pd.DataFrame({
        "Feature": X.columns,
        "Importance": best_model.feature_importances_
    }).sort_values(by="Importance", ascending=False)

plt.figure(figsize=(10,6))
sns.barplot(x="Importance", y="Feature", data=feat_imp.head(10))
plt.title(f"Top 10 Feature Importances - {best_name}")
plt.show()

# ============================================================
# 9Ô∏è‚É£ Save Predictions (for Power BI)
# ============================================================
y_pred = best_model.predict(X_test)
y_prob = best_model.predict_proba(X_test)[:,1]

df_pred = X_test.copy()
df_pred["CustomerId"] = id_test.values
df_pred["Actual_Churn"] = y_test.values
df_pred["Predicted_Churn"] = y_pred
df_pred["Churn_Probability"] = y_prob

df_pred.to_csv("churn_predictions.csv", index=False)
print("\n‚úÖ Saved predictions to churn_predictions.csv")

# ============================================================
# üîü Final Summary
# ============================================================
print(f"\nüî• Final Best Model: {best_name}")
print(f"üî• Accuracy: {results_df.iloc[0]['Accuracy']*100:.2f}%")
print(f"üî• AUC: {results_df.iloc[0]['AUC']:.3f}")